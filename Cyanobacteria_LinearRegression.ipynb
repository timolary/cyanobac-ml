{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "\n",
    "from data_manager import get_data\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        TP    DP    Cl    TN      TempC   Chla  Secchi   NP_Cya_bio  target  \\\n",
      "0     39.2  16.2  13.0  0.61   6.494521   1.41     0.5          0.0     0.0   \n",
      "1     36.8  14.8  17.5  0.45  13.700000   9.67     1.1          0.0     0.0   \n",
      "2     50.1  27.4  12.1  0.55  14.500000   2.04     0.7          0.0     0.0   \n",
      "4     59.6  32.6  12.0  0.65  17.700000   4.13     0.6          0.0     0.0   \n",
      "5     77.3  47.9  10.5  0.62  22.500000   1.74     0.6          0.0     0.0   \n",
      "...    ...   ...   ...   ...        ...    ...     ...          ...     ...   \n",
      "3629  53.4  16.8   8.0  0.69  25.600000  27.50     1.1  389000000.0     0.0   \n",
      "3631  83.4  33.9   8.3  0.71  23.700000  23.94     1.0  133000000.0     0.0   \n",
      "3632  94.2  40.7   8.7  0.90  22.300000  50.16     1.0  443000000.0     1.0   \n",
      "3634  68.8  42.6   9.6  0.74  13.400000  10.22     1.4    9460000.0     0.0   \n",
      "3636  79.4  49.7   9.2  0.76   9.200000  11.81     1.3    6510000.0     0.0   \n",
      "\n",
      "            N:P  Month  \n",
      "0     34.410892      4  \n",
      "1     27.040633      5  \n",
      "2     24.276000      5  \n",
      "4     24.116777      6  \n",
      "5     17.736355      6  \n",
      "...         ...    ...  \n",
      "3629  28.573274      8  \n",
      "3631  18.825411      9  \n",
      "3632  21.127289      9  \n",
      "3634  23.784578     10  \n",
      "3636  21.166315     10  \n",
      "\n",
      "[1431 rows x 11 columns]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "ds2, ds3 = get_data()\n",
    "print(ds3)\n",
    "print(len(ds3[ds3['target'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X and y\n",
    "X = np.array(ds3.drop(['target', 'NP_Cya_bio'], axis=1))\n",
    "y = np.array(ds3['target'])\n",
    "y_reg = np.array(ds3['NP_Cya_bio']) #for regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X, y_reg, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the X's\n",
    "X_train = preprocessing.scale(X_train)\n",
    "X_test = preprocessing.scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37390644585703214\n",
      "[ -336404.23193671  -399140.50445966 -1061222.48136934 46768878.00151783\n",
      "   143567.13433896 12134061.64218822  9344216.85477284    98500.58594958\n",
      "  1300100.67109838]\n"
     ]
    }
   ],
   "source": [
    "#Linear regression\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_reg_train, y_reg_train)\n",
    "#Testing\n",
    "print(model.score(X_reg_test, y_reg_test))\n",
    "print(model.coef_) # get theta coefficients (model params)\n",
    "\n",
    "#CV - doesn't really work, too much variation\n",
    "#r2 = cross_val_score(model, X, y_reg, scoring = 'r2', cv = 5, )\n",
    "#print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Results: Ordinary least squares\n",
      "================================================================================\n",
      "Model:                  OLS              Adj. R-squared (uncentered): 0.440     \n",
      "Dependent Variable:     y                AIC:                         56672.0654\n",
      "Date:                   2020-05-07 00:31 BIC:                         56719.4605\n",
      "No. Observations:       1431             Log-Likelihood:              -28327.   \n",
      "Df Model:               9                F-statistic:                 125.7     \n",
      "Df Residuals:           1422             Prob (F-statistic):          8.98e-174 \n",
      "R-squared (uncentered): 0.443            Scale:                       9.2091e+15\n",
      "---------------------------------------------------------------------------------\n",
      "         Coef.         Std.Err.       t     P>|t|       [0.025          0.975]   \n",
      "---------------------------------------------------------------------------------\n",
      "x1      36588.6634    429892.5307   0.0851  0.9322    -806702.9886    879880.3155\n",
      "x2    -962534.7944    660042.2441  -1.4583  0.1450   -2257295.8677    332226.2788\n",
      "x3   -3798249.7623    656333.5875  -5.7871  0.0000   -5085735.8100  -2510763.7145\n",
      "x4   -5650428.2866  26371636.9299  -0.2143  0.8304  -57381918.5657  46081061.9926\n",
      "x5   -1135182.6523    494780.8100  -2.2943  0.0219   -2105761.3363   -164603.9684\n",
      "x6   11800222.0504    476830.7255  24.7472  0.0000   10864854.8563  12735589.2445\n",
      "x7    5995352.7257   2136464.2347   2.8062  0.0051    1804392.6013  10186312.8501\n",
      "x8     -34974.0123    230952.2468  -0.1514  0.8797    -488017.7101    418069.6855\n",
      "x9     672682.6403   1606839.1501   0.4186  0.6755   -2479347.0989   3824712.3796\n",
      "--------------------------------------------------------------------------------\n",
      "Omnibus:                 2006.435          Durbin-Watson:             1.937     \n",
      "Prob(Omnibus):           0.000             Jarque-Bera (JB):          933219.895\n",
      "Skew:                    7.629             Prob(JB):                  0.000     \n",
      "Kurtosis:                127.172           Condition No.:             628       \n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Try again using other method? Yes, more info, p-values give feature significance!\n",
    "import statsmodels.api as sm\n",
    "# Fit regression model\n",
    "model = sm.OLS(y_reg,X)\n",
    "result = model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.3333333333333333\n",
      "ROC AUC: 0.6666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00       284\n",
      "         1.0       1.00      0.33      0.50         3\n",
      "\n",
      "    accuracy                           0.99       287\n",
      "   macro avg       1.00      0.67      0.75       287\n",
      "weighted avg       0.99      0.99      0.99       287\n",
      "\n",
      "[[284   0]\n",
      " [  2   1]]\n"
     ]
    }
   ],
   "source": [
    "#Basic logistic Regression\n",
    "model = LogisticRegression(solver = \"liblinear\", penalty = 'l1', C = 1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "print('Recall:', recall)\n",
    "print('ROC AUC:', auc)\n",
    "\n",
    "scores = metrics.classification_report(y_test, y_pred)\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(scores)\n",
    "print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD POLYNOMIAL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahaliaclark/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 60 is smaller than n_iter=800. Running 60 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  72 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    2.0s finished\n",
      "/Users/mahaliaclark/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 450 is smaller than n_iter=800. Running 450 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 450 candidates, totalling 2250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  40 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=4)]: Done 370 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=4)]: Done 460 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=4)]: Done 586 tasks      | elapsed:   48.4s\n",
      "[Parallel(n_jobs=4)]: Done 748 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done 946 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done 1180 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=4)]: Done 1450 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=4)]: Done 1756 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=4)]: Done 2098 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=4)]: Done 2250 out of 2250 | elapsed:  4.2min finished\n",
      "/Users/mahaliaclark/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                    multi_class='warn', n_jobs=None, penalty='l1',\n",
       "                    random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 0.9593686181075561,\n",
       " LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=0.5714285714285714,\n",
       "                    max_iter=1000, multi_class='warn', n_jobs=None,\n",
       "                    penalty='elasticnet', random_state=None, solver='saga',\n",
       "                    tol=0.0001, verbose=0, warm_start=False),\n",
       " 0.9596638612125338]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regression \n",
    "# - tune hyperparams to find the best model! Check out hw03\n",
    "# - ADD POLYNOMIAL FEATURES - see LectureProject_1\n",
    "\n",
    "#Ridge and Lasso\n",
    "model = LogisticRegression(solver = \"liblinear\", max_iter = 1000)\n",
    "\n",
    "#Use CV to find best parameters: \n",
    "best_estimators = []\n",
    "distros = dict(C = np.logspace(-2, 12, 15), \n",
    "               class_weight = ['balanced', None], \n",
    "               penalty = ['l1', 'l2'])\n",
    "\n",
    "search = RandomizedSearchCV(model, distros, scoring='roc_auc', refit='AUC', verbose=5, cv=5, n_iter=800, n_jobs=4, pre_dispatch='2*n_jobs')\n",
    "search = search.fit(X_train, y_train)\n",
    "best_estimators.append(search.best_estimator_)\n",
    "best_estimators.append(search.best_score_)\n",
    "\n",
    "#Elasticnet\n",
    "model = LogisticRegression(solver = \"saga\", penalty = 'elasticnet', max_iter = 1000)\n",
    "distros = dict(C = np.logspace(-2, 12, 15), \n",
    "               class_weight = ['balanced', None],\n",
    "              l1_ratio = np.linspace(0, 1, 15))\n",
    "search = RandomizedSearchCV(model, distros, scoring='roc_auc', refit='AUC', verbose=5, cv=5, n_iter=800, n_jobs=4, pre_dispatch='2*n_jobs')\n",
    "search = search.fit(X_train, y_train)\n",
    "best_estimators.append(search.best_estimator_)\n",
    "best_estimators.append(search.best_score_)\n",
    "best_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.3333333333333333\n",
      "ROC AUC: 0.6649061032863849\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99       284\n",
      "         1.0       0.50      0.33      0.40         3\n",
      "\n",
      "    accuracy                           0.99       287\n",
      "   macro avg       0.75      0.66      0.70       287\n",
      "weighted avg       0.99      0.99      0.99       287\n",
      "\n",
      "[[283   1]\n",
      " [  2   1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahaliaclark/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#They do equally well\n",
    "model1 = LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                    intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
    "                    multi_class='warn', n_jobs=None, penalty='l1',\n",
    "                    random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                    warm_start=False)\n",
    "model2 = LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                    intercept_scaling=1, l1_ratio=0.5714285714285714,\n",
    "                    max_iter=1000, multi_class='warn', n_jobs=None,\n",
    "                    penalty='elasticnet', random_state=None, solver='saga',\n",
    "                    tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Testing\n",
    "y_pred = model.predict(X_test)\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "print('Recall:', recall)\n",
    "print('ROC AUC:', auc)\n",
    "scores = metrics.classification_report(y_test, y_pred)\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(scores)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to try for logistic regression:\n",
    "- ds2 vs ds3\n",
    "- features scaled vs not\n",
    "- use cross validation\n",
    "- get performance summary with recall and ROC AUC and confusion matrix\n",
    "- **add polynomial features**\n",
    "- add regularization: try elastic net and/or Lasso (=L1?)\n",
    "- use random search and cross validation to tune hyperparameters, find best model (specify scoring metric as recall/F1/AUC?):\n",
    "    - C (inverse of regularization strength --> smaller value = more regularization)\n",
    "    - solver (choice depends on choice of regularization)\n",
    "    - penalty (form of regularization?)\n",
    "    - l1_ratio: ratio between 0 and 1 passed for elasticnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
