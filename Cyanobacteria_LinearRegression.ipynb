{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "\n",
    "from data_manager import get_data\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        TP    DP    Cl    TN      TempC   Chla  Secchi   NP_Cya_bio  target  \\\n",
      "0     39.2  16.2  13.0  0.61   6.494521   1.41     0.5          0.0     0.0   \n",
      "1     36.8  14.8  17.5  0.45  13.700000   9.67     1.1          0.0     0.0   \n",
      "2     50.1  27.4  12.1  0.55  14.500000   2.04     0.7          0.0     0.0   \n",
      "4     59.6  32.6  12.0  0.65  17.700000   4.13     0.6          0.0     0.0   \n",
      "5     77.3  47.9  10.5  0.62  22.500000   1.74     0.6          0.0     0.0   \n",
      "...    ...   ...   ...   ...        ...    ...     ...          ...     ...   \n",
      "3629  53.4  16.8   8.0  0.69  25.600000  27.50     1.1  389000000.0     0.0   \n",
      "3631  83.4  33.9   8.3  0.71  23.700000  23.94     1.0  133000000.0     0.0   \n",
      "3632  94.2  40.7   8.7  0.90  22.300000  50.16     1.0  443000000.0     1.0   \n",
      "3634  68.8  42.6   9.6  0.74  13.400000  10.22     1.4    9460000.0     0.0   \n",
      "3636  79.4  49.7   9.2  0.76   9.200000  11.81     1.3    6510000.0     0.0   \n",
      "\n",
      "            N:P  Month  \n",
      "0     34.410892      4  \n",
      "1     27.040633      5  \n",
      "2     24.276000      5  \n",
      "4     24.116777      6  \n",
      "5     17.736355      6  \n",
      "...         ...    ...  \n",
      "3629  28.573274      8  \n",
      "3631  18.825411      9  \n",
      "3632  21.127289      9  \n",
      "3634  23.784578     10  \n",
      "3636  21.166315     10  \n",
      "\n",
      "[1431 rows x 11 columns]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "ds2, ds3 = get_data()\n",
    "print(ds3)\n",
    "print(len(ds3[ds3['target'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X and y\n",
    "X = np.array(ds3.drop(['target', 'NP_Cya_bio'], axis=1))\n",
    "y = np.array(ds3['target'])\n",
    "y_reg = np.array(ds3['NP_Cya_bio']) #for regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 42)\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X, y_reg, test_size=0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the X's for logistic regression (didn't help for linear regression)\n",
    "X_train = preprocessing.scale(X_train)\n",
    "X_test = preprocessing.scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try adding polynomial features\n",
    "\n",
    "#Define Polynomial transformation \n",
    "poly = PolynomialFeatures(2, include_bias=False)\n",
    "\n",
    "#Map the features\n",
    "poly_X_train = poly.fit_transform(X_train)\n",
    "poly_X_test = poly.fit_transform(X_test)\n",
    "\n",
    "poly_Xr_train = poly.fit_transform(X_reg_train)\n",
    "poly_Xr_test = poly.fit_transform(X_reg_test)\n",
    "#print(poly_X_train.shape, poly_X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2966766257913154\n",
      "[  126475.2968059   -517697.54255332 -1304943.14617989 32969247.88086407\n",
      "  -205743.17908729 12238131.67368069 11312522.94010345   217084.49920213\n",
      "  1908673.49844075]\n",
      "0.42085912012181614\n",
      "[-4.33607466e+05  1.97232132e+06  1.24166427e+07 -6.65851496e+08\n",
      " -1.06236033e+07  8.00611714e+06 -7.69035755e+06  1.54813859e+06\n",
      "  1.97225206e+07 -1.38367122e+04  2.42157478e+05 -1.97075980e+05\n",
      " -1.86005015e+06 -9.41496094e+04 -2.64407251e+05 -4.87883686e+05\n",
      " -3.47452373e+03  7.40893805e+05 -1.66456211e+05  2.48169153e+05\n",
      " -6.25516534e+06  3.53042200e+05 -4.44816625e+05  3.18073790e+05\n",
      "  5.16334387e+04 -1.59704180e+06 -2.69593773e+04  2.47966742e+05\n",
      "  7.83502296e+04 -6.26347207e+05 -1.20386100e+06 -4.11771916e+03\n",
      " -6.16001196e+05  3.21433926e+08 -1.12271961e+07  4.74361463e+07\n",
      "  9.55392088e+07 -2.87281592e+06  4.32759541e+07 -1.36609238e+05\n",
      "  7.75879891e+05  4.91221837e+05  7.94201506e+04  8.88886103e+05\n",
      "  2.11625611e+04 -1.30850724e+06 -2.86074181e+05 -7.66776322e+05\n",
      "  4.20282550e+05 -3.44846230e+05  5.34356610e+04  7.13630455e+03\n",
      " -1.17449719e+05 -1.68406338e+06]\n"
     ]
    }
   ],
   "source": [
    "#Linear regression\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_reg_train, y_reg_train)\n",
    "#Testing\n",
    "print(model.score(X_reg_test, y_reg_test))\n",
    "print(model.coef_) # get theta coefficients (model params)\n",
    "\n",
    "#CV - doesn't really work, too much variation\n",
    "#r2 = cross_val_score(model, X, y_reg, scoring = 'r2', cv = 5, )\n",
    "\n",
    "\n",
    "#Linear regression w/ polynomial features\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(poly_Xr_train, y_reg_train)\n",
    "#Testing\n",
    "print(model.score(poly_Xr_test, y_reg_test))\n",
    "print(model.coef_) # get theta coefficients (model params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Results: Ordinary least squares\n",
      "================================================================================\n",
      "Model:                  OLS              Adj. R-squared (uncentered): 0.440     \n",
      "Dependent Variable:     y                AIC:                         56672.0654\n",
      "Date:                   2020-05-07 13:56 BIC:                         56719.4605\n",
      "No. Observations:       1431             Log-Likelihood:              -28327.   \n",
      "Df Model:               9                F-statistic:                 125.7     \n",
      "Df Residuals:           1422             Prob (F-statistic):          8.98e-174 \n",
      "R-squared (uncentered): 0.443            Scale:                       9.2091e+15\n",
      "---------------------------------------------------------------------------------\n",
      "         Coef.         Std.Err.       t     P>|t|       [0.025          0.975]   \n",
      "---------------------------------------------------------------------------------\n",
      "x1      36588.6634    429892.5307   0.0851  0.9322    -806702.9886    879880.3155\n",
      "x2    -962534.7944    660042.2441  -1.4583  0.1450   -2257295.8677    332226.2788\n",
      "x3   -3798249.7623    656333.5875  -5.7871  0.0000   -5085735.8100  -2510763.7145\n",
      "x4   -5650428.2866  26371636.9299  -0.2143  0.8304  -57381918.5657  46081061.9926\n",
      "x5   -1135182.6523    494780.8100  -2.2943  0.0219   -2105761.3363   -164603.9684\n",
      "x6   11800222.0504    476830.7255  24.7472  0.0000   10864854.8563  12735589.2445\n",
      "x7    5995352.7257   2136464.2347   2.8062  0.0051    1804392.6013  10186312.8501\n",
      "x8     -34974.0123    230952.2468  -0.1514  0.8797    -488017.7101    418069.6855\n",
      "x9     672682.6403   1606839.1501   0.4186  0.6755   -2479347.0989   3824712.3796\n",
      "--------------------------------------------------------------------------------\n",
      "Omnibus:                 2006.435          Durbin-Watson:             1.937     \n",
      "Prob(Omnibus):           0.000             Jarque-Bera (JB):          933219.895\n",
      "Skew:                    7.629             Prob(JB):                  0.000     \n",
      "Kurtosis:                127.172           Condition No.:             628       \n",
      "================================================================================\n",
      "\n",
      "                         Results: Ordinary least squares\n",
      "=================================================================================\n",
      "Model:                   OLS              Adj. R-squared (uncentered): 0.677     \n",
      "Dependent Variable:      y                AIC:                         44768.8894\n",
      "Date:                    2020-05-07 13:56 BIC:                         45041.1729\n",
      "No. Observations:        1144             Log-Likelihood:              -22330.   \n",
      "Df Model:                54               F-statistic:                 45.35     \n",
      "Df Residuals:            1090             Prob (F-statistic):          8.72e-239 \n",
      "R-squared (uncentered):  0.692            Scale:                       5.5342e+15\n",
      "---------------------------------------------------------------------------------\n",
      "         Coef.         Std.Err.       t    P>|t|       [0.025          0.975]    \n",
      "---------------------------------------------------------------------------------\n",
      "x1      -19779.7292   4686538.4583 -0.0042 0.9966    -9215437.2241   9175877.7658\n",
      "x2     1871376.8978   7371365.5085  0.2539 0.7996   -12592294.5496  16335048.3452\n",
      "x3    13597319.8984   6633043.0459  2.0499 0.0406      582342.5239  26612297.2728\n",
      "x4  -656662961.7930 289778251.9959 -2.2661 0.0236 -1225249260.3087 -88076663.2773\n",
      "x5   -10412670.9061   4995019.8467 -2.0846 0.0373   -20213612.9233   -611728.8890\n",
      "x6     8162263.5843   5642792.4557  1.4465 0.1483    -2909700.7664  19234227.9349\n",
      "x7    -4889839.4125  20502278.3081 -0.2385 0.8115   -45118236.3127  35338557.4877\n",
      "x8     1750148.3735   2126146.4693  0.8232 0.4106    -2421654.5232   5921951.2701\n",
      "x9    23656451.3496  23113691.7414  1.0235 0.3063   -21695911.5064  69008814.2055\n",
      "x10     -14393.4043     27436.6632 -0.5246 0.6000      -68228.0542     39441.2457\n",
      "x11     241326.3564     71165.0002  3.3911 0.0007      101690.4668    380962.2461\n",
      "x12    -201823.1220    137315.9114 -1.4698 0.1419     -471256.5430     67610.2990\n",
      "x13   -1882522.9272   3815679.0980 -0.4934 0.6219    -9369430.0354   5604384.1809\n",
      "x14     -99203.7711    109399.8024 -0.9068 0.3647     -313861.8012    115454.2589\n",
      "x15    -264755.6179     60719.5711 -4.3603 0.0000     -383896.0847   -145615.1512\n",
      "x16    -503098.5365    505108.5057 -0.9960 0.3195    -1494193.5328    487996.4597\n",
      "x17      -4106.1047     60861.2392 -0.0675 0.9462     -123524.5444    115312.3350\n",
      "x18     727839.0946    383791.1449  1.8964 0.0582      -25213.9208   1480892.1100\n",
      "x19    -166338.7939     68203.1218 -2.4389 0.0149     -300163.0554    -32514.5325\n",
      "x20     247234.0848    175561.2908  1.4082 0.1593      -97242.2305    591710.4002\n",
      "x21   -6160836.9028   5161854.5802 -1.1935 0.2329   -16289132.4841   3967458.6785\n",
      "x22     358260.8610    155177.9533  2.3087 0.0211       53779.5638    662742.1582\n",
      "x23    -444246.3134     80928.1131 -5.4894 0.0000     -603038.8244   -285453.8024\n",
      "x24     315411.6226    734065.3671  0.4297 0.6675    -1124929.4212   1755752.6665\n",
      "x25      50528.3835     90168.5346  0.5604 0.5753     -126395.1536    227451.9207\n",
      "x26   -1595412.9634    492765.6565 -3.2377 0.0012    -2562289.5275   -628536.3993\n",
      "x27     -37665.0234    172895.5254 -0.2178 0.8276     -376910.7265    301580.6796\n",
      "x28      72305.2250   8580307.0026  0.0084 0.9933   -16763482.0198  16908092.4697\n",
      "x29      72836.9248    150490.2741  0.4840 0.6285     -222446.4767    368120.3263\n",
      "x30    -628926.3332    153910.8468 -4.0863 0.0000     -930921.3866   -326931.2797\n",
      "x31   -1264991.7670    876650.8319 -1.4430 0.1493    -2985105.8479    455122.3140\n",
      "x32      -9109.1880     87045.4438 -0.1046 0.9167     -179904.7751    161686.3990\n",
      "x33    -640173.7189    461380.1453 -1.3875 0.1656    -1545467.4295    265119.9917\n",
      "x34  315827993.5545 147591468.4612  2.1399 0.0326    26232462.6034 605423524.5057\n",
      "x35  -11313336.1994   5886017.9476 -1.9221 0.0549   -22862543.6874    235871.2887\n",
      "x36   47444406.9964   4720039.4040 10.0517 0.0000    38183015.8634  56705798.1294\n",
      "x37   94906623.5127  33263632.8446  2.8532 0.0044    29638627.2293 160174619.7961\n",
      "x38   -2801627.9976   3091355.9176 -0.9063 0.3650    -8867309.6231   3264053.6279\n",
      "x39   42917484.7757  24895017.6910  1.7239 0.0850    -5930093.8915  91765063.4429\n",
      "x40    -135510.3431    100103.2913 -1.3537 0.1761     -331927.2910     60906.6049\n",
      "x41     775665.4970    131625.1795  5.8930 0.0000      517398.1043   1033932.8896\n",
      "x42     475641.5929    450798.3349  1.0551 0.2916     -408889.0952   1360172.2810\n",
      "x43      78080.9843     53199.7850  1.4677 0.1425      -26304.5885    182466.5571\n",
      "x44     876661.2219    390173.6968  2.2468 0.0248      111084.7284   1642237.7154\n",
      "x45      21133.6913     23586.7522  0.8960 0.3705      -25146.8838     67414.2664\n",
      "x46   -1308440.5947    699197.2138 -1.8713 0.0616    -2680365.3440     63484.1545\n",
      "x47    -287375.8281     76651.5008 -3.7491 0.0002     -437777.0154   -136974.6409\n",
      "x48    -777296.2511    439689.5639 -1.7678 0.0774    -1640029.9445     85437.4424\n",
      "x49     367405.1093   1347472.9182  0.2727 0.7852    -2276529.1184   3011339.3370\n",
      "x50    -354213.0230    223650.2423 -1.5838 0.1135     -793046.7262     84620.6803\n",
      "x51      15608.5555   1371953.8486  0.0114 0.9909    -2676360.7525   2707577.8635\n",
      "x52       6806.5851     16904.2246  0.4027 0.6873      -26361.9168     39975.0869\n",
      "x53    -122587.5851    183206.1819 -0.6691 0.5036     -482064.2681    236889.0979\n",
      "x54   -1851521.2211   1427230.9775 -1.2973 0.1948    -4651952.1474    948909.7051\n",
      "---------------------------------------------------------------------------------\n",
      "Omnibus:                  1183.843          Durbin-Watson:             2.000     \n",
      "Prob(Omnibus):            0.000             Jarque-Bera (JB):          249015.228\n",
      "Skew:                     4.403             Prob(JB):                  0.000     \n",
      "Kurtosis:                 74.739            Condition No.:             530034    \n",
      "=================================================================================\n",
      "* The condition number is large (5e+05). This might indicate             strong\n",
      "multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "#Try again using other method? Uses all data, gives p-values give feature significance!\n",
    "import statsmodels.api as sm\n",
    "# Fit regression model\n",
    "model = sm.OLS(y_reg,X)\n",
    "result = model.fit()\n",
    "print(result.summary2())\n",
    "\n",
    "#And with polynomial features:\n",
    "model = sm.OLS(y_reg_train, poly_Xr_train)\n",
    "result = model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.25\n",
      "ROC AUC: 0.6214664310954063\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       283\n",
      "         1.0       0.33      0.25      0.29         4\n",
      "\n",
      "    accuracy                           0.98       287\n",
      "   macro avg       0.66      0.62      0.64       287\n",
      "weighted avg       0.98      0.98      0.98       287\n",
      "\n",
      "[[281   2]\n",
      " [  3   1]]\n"
     ]
    }
   ],
   "source": [
    "#Basic logistic Regression\n",
    "model = LogisticRegression(solver = \"liblinear\", penalty = 'l1', C = 1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "print('Recall:', recall)\n",
    "print('ROC AUC:', auc)\n",
    "\n",
    "scores = metrics.classification_report(y_test, y_pred)\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(scores)\n",
    "print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5\n",
      "ROC AUC: 0.7464664310954063\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       283\n",
      "         1.0       0.50      0.50      0.50         4\n",
      "\n",
      "    accuracy                           0.99       287\n",
      "   macro avg       0.75      0.75      0.75       287\n",
      "weighted avg       0.99      0.99      0.99       287\n",
      "\n",
      "[[281   2]\n",
      " [  2   2]]\n"
     ]
    }
   ],
   "source": [
    "#Basic Logistic regression plus polynomial features:\n",
    "model = LogisticRegression(solver = \"liblinear\", penalty = 'l1', C = 1)\n",
    "model.fit(poly_X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(poly_X_test)\n",
    "\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "print('Recall:', recall)\n",
    "print('ROC AUC:', auc)\n",
    "\n",
    "scores = metrics.classification_report(y_test, y_pred)\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(scores)\n",
    "print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahaliaclark/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 14 is smaller than n_iter=800. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=4)]: Done  70 out of  70 | elapsed:    3.1s finished\n",
      "/Users/mahaliaclark/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 14 is smaller than n_iter=800. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  70 out of  70 | elapsed:    0.5s finished\n",
      "/Users/mahaliaclark/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/mahaliaclark/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 98 is smaller than n_iter=800. Running 98 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 98 candidates, totalling 490 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  60 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 490 out of 490 | elapsed:  2.1min finished\n",
      "/Users/mahaliaclark/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=0.001, class_weight='balanced', dual=False,\n",
       "                    fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                    max_iter=10000, multi_class='warn', n_jobs=None,\n",
       "                    penalty='l2', random_state=None, solver='liblinear',\n",
       "                    tol=0.0001, verbose=0, warm_start=False),\n",
       " 0.9636835595574533,\n",
       " LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                    multi_class='warn', n_jobs=None, penalty='l1',\n",
       "                    random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 0.9540243139358185,\n",
       " LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=0.1, max_iter=10000,\n",
       "                    multi_class='warn', n_jobs=None, penalty='elasticnet',\n",
       "                    random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 0.961996255956433]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Basic logistic regression - tune hyperparams to find the best model! \n",
    "\n",
    "#Ridge\n",
    "model = LogisticRegression(solver = \"liblinear\", max_iter = 10000, penalty = 'l2')\n",
    "\n",
    "#Use CV to find best parameters: \n",
    "best_estimators = []\n",
    "distros = dict(C = np.logspace(-4, 2, 7), \n",
    "               class_weight = ['balanced', None])\n",
    "\n",
    "search = RandomizedSearchCV(model, distros, scoring='roc_auc', refit='AUC', verbose=5, cv=5, n_iter=800, n_jobs=4, pre_dispatch='2*n_jobs')\n",
    "search = search.fit(X_train, y_train)\n",
    "best_estimators.append(search.best_estimator_)\n",
    "best_estimators.append(search.best_score_)\n",
    "\n",
    "#Lasso\n",
    "model = LogisticRegression(solver = \"liblinear\", max_iter = 10000, penalty = 'l1')\n",
    "distros = dict(C = np.logspace(-4, 2, 7), \n",
    "               class_weight = ['balanced', None])\n",
    "\n",
    "search = RandomizedSearchCV(model, distros, scoring='roc_auc', refit='AUC', verbose=5, cv=5, n_iter=800, n_jobs=4, pre_dispatch='2*n_jobs')\n",
    "search = search.fit(X_train, y_train)\n",
    "best_estimators.append(search.best_estimator_)\n",
    "best_estimators.append(search.best_score_)\n",
    "\n",
    "#Elasticnet\n",
    "model = LogisticRegression(solver = \"saga\", penalty = 'elasticnet', max_iter = 10000)\n",
    "distros = dict(C = np.logspace(-4, 2, 7), \n",
    "               class_weight = ['balanced', None],\n",
    "              l1_ratio = [.1, .5, .7, .9, .95, .99, 1])\n",
    "search = RandomizedSearchCV(model, distros, scoring='roc_auc', refit='AUC', verbose=5, cv=5, n_iter=800, n_jobs=4, pre_dispatch='2*n_jobs')\n",
    "search = search.fit(X_train, y_train)\n",
    "best_estimators.append(search.best_estimator_)\n",
    "best_estimators.append(search.best_score_)\n",
    "best_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahaliaclark/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 14 is smaller than n_iter=800. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=4)]: Done  70 out of  70 | elapsed:    3.2s finished\n",
      "/Users/mahaliaclark/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/mahaliaclark/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 14 is smaller than n_iter=800. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  70 out of  70 | elapsed:    7.4s finished\n",
      "/Users/mahaliaclark/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/mahaliaclark/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 98 is smaller than n_iter=800. Running 98 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 98 candidates, totalling 490 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done 265 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=4)]: Done 372 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=4)]: Done 490 out of 490 | elapsed:  8.3min finished\n",
      "/Users/mahaliaclark/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=0.001, class_weight='balanced', dual=False,\n",
       "                    fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                    max_iter=10000, multi_class='warn', n_jobs=None,\n",
       "                    penalty='l2', random_state=None, solver='liblinear',\n",
       "                    tol=0.0001, verbose=0, warm_start=False),\n",
       " 0.9636835595574533,\n",
       " LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                    multi_class='warn', n_jobs=None, penalty='l1',\n",
       "                    random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 0.9540243139358185,\n",
       " LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=0.1, max_iter=10000,\n",
       "                    multi_class='warn', n_jobs=None, penalty='elasticnet',\n",
       "                    random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 0.961996255956433,\n",
       " LogisticRegression(C=0.0001, class_weight='balanced', dual=False,\n",
       "                    fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                    max_iter=10000, multi_class='warn', n_jobs=None,\n",
       "                    penalty='l2', random_state=None, solver='liblinear',\n",
       "                    tol=0.0001, verbose=0, warm_start=False),\n",
       " 0.9644235324655678,\n",
       " LogisticRegression(C=0.001, class_weight='balanced', dual=False,\n",
       "                    fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                    max_iter=10000, multi_class='warn', n_jobs=None,\n",
       "                    penalty='l1', random_state=None, solver='liblinear',\n",
       "                    tol=0.0001, verbose=0, warm_start=False),\n",
       " 0.9157327014185421,\n",
       " LogisticRegression(C=0.001, class_weight='balanced', dual=False,\n",
       "                    fit_intercept=True, intercept_scaling=1, l1_ratio=0.7,\n",
       "                    max_iter=10000, multi_class='warn', n_jobs=None,\n",
       "                    penalty='elasticnet', random_state=None, solver='saga',\n",
       "                    tol=0.0001, verbose=0, warm_start=False),\n",
       " 0.9157327014185421]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression with polynomial features - tune hyperparams to find the best model\n",
    "\n",
    "#Ridge\n",
    "model = LogisticRegression(solver = \"liblinear\", max_iter = 10000, penalty = 'l2')\n",
    "\n",
    "#Use CV to find best parameters: \n",
    "#best_estimators = []\n",
    "distros = dict(C = np.logspace(-4, 2, 7), \n",
    "               class_weight = ['balanced', None])\n",
    "\n",
    "search = RandomizedSearchCV(model, distros, scoring='roc_auc', refit='AUC', verbose=5, cv=5, n_iter=800, n_jobs=4, pre_dispatch='2*n_jobs')\n",
    "search = search.fit(poly_X_train, y_train)\n",
    "best_estimators.append(search.best_estimator_)\n",
    "best_estimators.append(search.best_score_)\n",
    "\n",
    "#Lasso\n",
    "model = LogisticRegression(solver = \"liblinear\", max_iter = 10000, penalty = 'l1')\n",
    "distros = dict(C = np.logspace(-4, 2, 7), \n",
    "               class_weight = ['balanced', None])\n",
    "\n",
    "search = RandomizedSearchCV(model, distros, scoring='roc_auc', refit='AUC', verbose=5, cv=5, n_iter=800, n_jobs=4, pre_dispatch='2*n_jobs')\n",
    "search = search.fit(poly_X_train, y_train)\n",
    "best_estimators.append(search.best_estimator_)\n",
    "best_estimators.append(search.best_score_)\n",
    "\n",
    "#Elasticnet\n",
    "model = LogisticRegression(solver = \"saga\", penalty = 'elasticnet', max_iter = 10000)\n",
    "distros = dict(C = np.logspace(-4, 2, 7), \n",
    "               class_weight = ['balanced', None],\n",
    "              l1_ratio = [.1, .5, .7, .9, .95, .99, 1])\n",
    "search = RandomizedSearchCV(model, distros, scoring='roc_auc', refit='AUC', verbose=5, cv=5, n_iter=800, n_jobs=4, pre_dispatch='2*n_jobs')\n",
    "search = search.fit(poly_X_train, y_train)\n",
    "best_estimators.append(search.best_estimator_)\n",
    "best_estimators.append(search.best_score_)\n",
    "best_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 1.0\n",
      "ROC AUC: 0.8621908127208481\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.72      0.84       283\n",
      "         1.0       0.05      1.00      0.09         4\n",
      "\n",
      "    accuracy                           0.73       287\n",
      "   macro avg       0.52      0.86      0.47       287\n",
      "weighted avg       0.99      0.73      0.83       287\n",
      "\n",
      "[[205  78]\n",
      " [  0   4]]\n"
     ]
    }
   ],
   "source": [
    "#Model 1 with Ridge (L2) does the best without polynomial features\n",
    "model = LogisticRegression(C=0.001, class_weight='balanced', dual=False,\n",
    "                    fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
    "                    max_iter=10000, multi_class='warn', n_jobs=None,\n",
    "                    penalty='l2', random_state=None, solver='liblinear',\n",
    "                    tol=0.0001, verbose=0, warm_start=False)\n",
    "model2 = LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                    intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
    "                    multi_class='warn', n_jobs=None, penalty='l1',\n",
    "                    random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                    warm_start=False)\n",
    "model3 = LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
    "                    intercept_scaling=1, l1_ratio=0.1, max_iter=10000,\n",
    "                    multi_class='warn', n_jobs=None, penalty='elasticnet',\n",
    "                    random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
    "                    warm_start=False)\n",
    "\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "#Testing\n",
    "y_pred = model1.predict(X_test)\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "print('Recall:', recall)\n",
    "print('ROC AUC:', auc)\n",
    "scores = metrics.classification_report(y_test, y_pred)\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(scores)\n",
    "print(confusion_matrix)\n",
    "\n",
    "#Results: Recall, ROC AUC for\n",
    "#model1: 1, 0.86 \n",
    "#model2: 0.25, 0.62\n",
    "#model3: 0.25, 0.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 1.0\n",
      "ROC AUC: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       283\n",
      "         1.0       0.01      1.00      0.03         4\n",
      "\n",
      "    accuracy                           0.01       287\n",
      "   macro avg       0.01      0.50      0.01       287\n",
      "weighted avg       0.00      0.01      0.00       287\n",
      "\n",
      "[[  0 283]\n",
      " [  0   4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahaliaclark/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Models with polynomial features:\n",
    "#Mixed results depending whether we care more about recall or ROC AUC\n",
    "model1 = LogisticRegression(C=0.0001, class_weight='balanced', dual=False,\n",
    "                    fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
    "                    max_iter=10000, multi_class='warn', n_jobs=None,\n",
    "                    penalty='l2', random_state=None, solver='liblinear',\n",
    "                    tol=0.0001, verbose=0, warm_start=False)\n",
    "model2 = LogisticRegression(C=0.001, class_weight='balanced', dual=False,\n",
    "                    fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
    "                    max_iter=10000, multi_class='warn', n_jobs=None,\n",
    "                    penalty='l1', random_state=None, solver='liblinear',\n",
    "                    tol=0.0001, verbose=0, warm_start=False)\n",
    "model3 = LogisticRegression(C=0.001, class_weight='balanced', dual=False,\n",
    "                    fit_intercept=True, intercept_scaling=1, l1_ratio=0.7,\n",
    "                    max_iter=10000, multi_class='warn', n_jobs=None,\n",
    "                    penalty='elasticnet', random_state=None, solver='saga',\n",
    "                    tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "model2.fit(poly_X_train, y_train)\n",
    "\n",
    "#Testing\n",
    "y_pred = model2.predict(poly_X_test)\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "print('Recall:', recall)\n",
    "print('ROC AUC:', auc)\n",
    "scores = metrics.classification_report(y_test, y_pred)\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(scores)\n",
    "print(confusion_matrix)\n",
    "\n",
    "#Results: Recall, ROC AUC for\n",
    "#model1: 0.75, 0.78\n",
    "#model2: 1, 0.5\n",
    "#model3: 0.5, 0.74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to try for logistic regression:\n",
    "- ds2 vs ds3\n",
    "- features scaled vs not\n",
    "- use cross validation\n",
    "- get performance summary with recall and ROC AUC and confusion matrix\n",
    "- **add polynomial features**\n",
    "- add regularization: try elastic net and/or Lasso (=L1?)\n",
    "- use random search and cross validation to tune hyperparameters, find best model (specify scoring metric as recall/F1/AUC?):\n",
    "    - C (inverse of regularization strength --> smaller value = more regularization)\n",
    "    - solver (choice depends on choice of regularization)\n",
    "    - penalty (form of regularization?)\n",
    "    - l1_ratio: ratio between 0 and 1 passed for elasticnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
